{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0v4-4cHwN5z9"
      },
      "source": [
        "Given code: \n",
        "1. import packages\n",
        "2. load image pairs img1 & img2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxIiSV-yN5jv"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load image pairs - you can change file names\n",
        "image1 = cv2.imread(\"/Users/kimjunho/Desktop/컴퓨터비전3-1/[CV]A2/IMG_7577.png\") #src\n",
        "image2 = cv2.imread(\"/Users/kimjunho/Desktop/컴퓨터비전3-1/[CV]A2/IMG_7578.png\") #dst"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GS-5N7zbN5Rs"
      },
      "source": [
        "Problem 1: [code by yourself]\n",
        "\n",
        "Define a function `get_transform_from_keypoints()` that computes the 3x3 homogeneous transform H from keypoint matches with the following input parameters and return variables:\n",
        "* img_src (input): image that is warped to be aligned to img_dst \n",
        "* img_dst (input): reference image to align img_src\n",
        "* H (output/return): computed linear transform matrix\n",
        "* kpts_src (output/return): computed keypoints for img_src\n",
        "* dscrpt_src (output/return): computed descriptors for img_src\n",
        "* kpts_dst (output/return): computed keypoints for img_dst\n",
        "* dscrpt_dst (output/return): computed descriptors for img_dst\n",
        "* matches (output/return): keypoint matches determined from SIFT\n",
        "\n",
        "When computing the linear transform, follow this proceses:\n",
        "* detect SIFT keypoints and compute SIFT descriptors\n",
        "* find the linear transform matrix H using the matched keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IheSHuUN5CR"
      },
      "outputs": [],
      "source": [
        "def get_transform_from_keypoints(img_src, img_dst):\n",
        "    \n",
        "    # Create a SIFT object and detect keypoints and descriptors for each image\n",
        "    sift = cv2.xfeatures2d.SIFT_create() \n",
        "    kpts_src, dscrpt_src = sift.detectAndCompute(img_src, None)\n",
        "    kpts_dst, dscrpt_dst = sift.detectAndCompute(img_dst, None)\n",
        "    \n",
        "    # Match the descriptors\n",
        "    matcher = cv2.FlannBasedMatcher_create()\n",
        "    matches = matcher.match(dscrpt_src, dscrpt_dst)\n",
        "    matches = sorted(matches, key = lambda x:x.distance)\n",
        "    matches = matches[0:50]\n",
        "\n",
        "    # Find the homography matrix using the matched keypoints\n",
        "\n",
        "    src_pts = []\n",
        "    dst_pts = []\n",
        "    for match in matches:\n",
        "        src_pts.append(kpts_src[match.queryIdx].pt)\n",
        "        dst_pts.append(kpts_dst[match.trainIdx].pt)\n",
        "    src_pts = np.array(src_pts, dtype=np.float32).reshape(-1, 1, 2)\n",
        "    print('src_pts.shape = ', src_pts.shape) #50, 1, 2\n",
        "    dst_pts = np.array(dst_pts, dtype=np.float32).reshape(-1, 1, 2)\n",
        "    print('dst_pts.shape = ', dst_pts.shape) #50, 1, 2\n",
        "    H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
        "\n",
        "    \n",
        "    return H, kpts_src, dscrpt_src, kpts_dst, dscrpt_dst, matches"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VLa2-PdCN4sj"
      },
      "source": [
        "Problem 2: [Code by yourself]\n",
        "\n",
        "Define a function named `stitch_image()` that generates a stitched image from img_ref, img_align, and T, with the following input parameters and return variables:\n",
        "* img_src (input): image that is warped to be aligned to img_dst \n",
        "* img_dst (input): reference image to align img_src\n",
        "* H (input): computed linear transform matrix\n",
        "* stitched_image (output): the stitched image\n",
        "\n",
        "This function should include the following processes:\n",
        "* Compute the size of the output stitched image and the offset that ensures all pixels from both images are included in the stitched image\n",
        "* Modify H to account for the image offset\n",
        "* Warp the first src image into the second image\n",
        "* Blend in the second dst image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-eV8gu5NwAC"
      },
      "outputs": [],
      "source": [
        "def get_stitched_image(img_src, img_dst, H):\n",
        "\n",
        "    # Compute the size of the output stitched image\n",
        "    src_h, src_w = img_src.shape[:2]\n",
        "    dst_h, dst_w = img_dst.shape[:2]\n",
        "    src_corners = np.array([[0, 0, 1], [0, src_h, 1], [src_w, src_h, 1], [src_w, 0, 1]], dtype=np.float32)\n",
        "    warped_corners = np.dot(H, src_corners.T).T\n",
        "    warped_corners[:, 0] /= warped_corners[:, 2]\n",
        "    warped_corners[:, 1] /= warped_corners[:, 2]\n",
        "    min_x = int(np.floor(np.min(warped_corners[:, 0])))\n",
        "    max_x = int(np.ceil(np.max(warped_corners[:, 0])))\n",
        "    min_y = int(np.floor(np.min(warped_corners[:, 1])))\n",
        "    max_y = int(np.ceil(np.max(warped_corners[:, 1])))\n",
        "\n",
        "    # Modify H to account for the image offset\n",
        "    stitched_w = max(max_x, dst_w) - min(min_x, 0)\n",
        "    stitched_h = max(max_y, dst_h) - min(min_y, 0)\n",
        "    x_offset = min(min_x, 0)\n",
        "    y_offset = min(min_y, 0)\n",
        "    H_offset = np.array([[1, 0, -x_offset], [0, 1, -y_offset], [0, 0, 1]], dtype=np.float32)\n",
        "    H_modified = np.dot(H_offset, H)\n",
        "\n",
        "    # Warp the first image to the perspective of the second image\n",
        "    warped_img = cv2.warpPerspective(img_src, H_modified, (stitched_w, stitched_h))\n",
        "    cv2.imshow('warped_img', warped_img)\n",
        "    \n",
        "    # Combine the two images to create a single stitched image\n",
        "    stitched_image = np.zeros((stitched_h, stitched_w, img_dst.shape[2]), dtype=np.uint8)\n",
        "    stitched_image[-y_offset:-y_offset+dst_h, -x_offset:-x_offset+dst_w] = img_dst\n",
        "    stitched_image = cv2.addWeighted(stitched_image, 0.5, warped_img, 0.5, 0.0)\n",
        "\n",
        "    # return output\n",
        "    return stitched_image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "94CDChs3gh5V"
      },
      "source": [
        "Given code: \n",
        "3. Call function `get_transform_from_keypoints` - detect and match keypoints and compute transform\n",
        "4. Draw the matches on a new image to check validity of matched keypoints\n",
        "5. Call function `get_stitched_image` - create stitched image and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU2wERZJgiDq"
      },
      "outputs": [],
      "source": [
        "# 3. Detect and match keypoints and compute transform\n",
        "H, kpts_src, _, kpts_dst, _, matches = get_transform_from_keypoints(image1, image2)\n",
        "\n",
        "# 4. Draw the matches on a new image to check validity of matched keypoints\n",
        "match_image = cv2.drawMatches(image1, kpts_src, image2, kpts_dst, matches, None)\n",
        "cv2.imwrite('matches.png', match_image)\n",
        "\n",
        "# 3. Create stitched image and save\n",
        "stitched_image = get_stitched_image(image1, image2, H)\n",
        "cv2.imwrite('stitched.png', stitched_image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 추가 과제\n",
        " - 본 파트에서는 구현한 get_transform_from_keypoints 함수의 결과물을 바탕으로, match가 잘 이루어졌는지를 평가합니다.\n",
        " - get_transform_from_keypoints 를 올바르게 구현되었을 경우 실행했을때 올바른 결과물이 나옵니다.\n",
        " - 만약 모듈 관련 에러가 발생할 경우, 아래의 코드를 실행하여 모듈을 설치해주세요.\n",
        "    - pip3 install sckit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from eval import evaluate_correspondence\n",
        "import os\n",
        "\n",
        "imgs = os.listdir(\"NotreDame\")\n",
        "img_src = cv2.imread(os.path.join(\"NotreDame\", \"NotreDame1.jpg\"))\n",
        "img_dst = cv2.imread(os.path.join(\"NotreDame\", \"NotreDame2.jpg\"))\n",
        "\n",
        "_, kpts_src, _, kpts_dst, _, matches =  get_transform_from_keypoints(img_src, img_dst)\n",
        "ground_truth_correspondence_file = os.path.join(\"NotreDame\", \"NotreDameEval.mat\")\n",
        "matches = sorted(matches, key = lambda x:x.distance)\n",
        "evaluate_correspondence(img_src, img_dst, ground_truth_correspondence_file, kpts_src, kpts_dst, matches, 10000, filename=\"notre_dame_matches.jpg\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
